
#### concatenate, stack
```python
import numpy as np


a = np.array([[1, 2], [3, 4]])
b = np.array([[5, 6], [7, 8]])
c = np.array([[5, 6], [7, 8], [9, 10]])

# concatenate axis=1은 가로(행), axis=0은 세로(열)로 붙음
print(np.concatenate((a, b), axis=0))
print('--------')
# stack은 1차원을 더 생성해서 놓는 것 (배열을 새로운 축으로 합침)
print(np.stack((a, b), axis=0))
```


#### 차원 변경

```python
a = np.array([[1, 2], [3, 4]])
print(a.ravel())
print(a.reshape(-1))
print(a.flatten())

# [1 2 3 4]
# [1 2 3 4]
# [1 2 3 4]
```


#### 범주형 컬럼을 N차원으로 변환

```python
categorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]
caetgorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]
print(caetgorical_embedding_sizes)

# [(4, 2), (4, 2), (4, 2), (3, 2), (3, 2), (3, 2)]
```


#### 파이토치 모델 전체

```python
class Model(nn.Module):
    def __init__(self, embedding_size, output_size, layers, p=0.4):
        super().__init__()
        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])
        self.embedding_dropout = nn.Dropout(p)
        
        all_layers = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        input_size = num_categorical_cols
        
        for i in layers:
            all_layers.append(nn.Linear(input_size, i))
            all_layers.append(nn.ReLU(inplace=True))
            all_layers.append(nn.BatchNorm1d(i))
            all_layers.append(nn.Dropout(p))
            input_size = i
            
            all_layers.append(nn.Linear(layers[-1], output_size))
            self.layers = nn.Sequqential(*all_layers)
            
    
    def forward(self, x_categorical):
        embeddings = []
        for i, e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:, i]))
            x = torch.cat(embeddings, 1)
            x = self.embedding_dropout(x)
            x = self.layers(x)
            
            return x
```

• self: 첫 번째 파라미터는 self를 지정해야 하며 자기 자신을 의미
EX) ex라는 함수가 있을 때 self 의미는 다음 그림과 같음
• ⓑ embedding_size: 범주형 칼럼의 임베딩 크기
• ⓒ output_size: 출력층의 크기
• ⓓ layers: 모든 계층에 대한 목록
• ⓔ p: 드롭아웃(기본값은 0.5)


### Tensorflow

#### 층: 딥러닝의 구성 요소
층은 하나 이상의 텐서를 입력으로 받고 하나 이상의 텐서를 출력
weight라는 층의 상태 포함(없을 수도 있음)
가중치는 확률적 경사 하강법으로 학습되는 하나 이상의 텐서이며 여기에 신경망이 학습한 지식이 담겨 있음


