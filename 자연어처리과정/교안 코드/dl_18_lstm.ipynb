{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4, 1) (6,)\n"
     ]
    }
   ],
   "source": [
    "#Sequence data\n",
    "X = np.array([[0,1,2,3],\n",
    "              [1,2,3,4],\n",
    "              [2,3,4,5],\n",
    "              [3,4,5,6],\n",
    "              [4,5,6,7],\n",
    "              [5,6,7,8]], dtype=np.float32)\n",
    "\n",
    "x_data = tf.reshape(X, (-1,4,1))\n",
    "\n",
    "y_data = np.array([4,5,6,7,8,9], dtype=np.float32)\n",
    "\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grades = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        # Wx : (D, 4*h), Wh: (h, 4*h)\n",
    "        \n",
    "        n, h = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x, Wx) + np.dot(Wh, h_prev) + b #(n, 4*h)\n",
    "        \n",
    "        # i, g, f, o 게이트 : 동일한 크기의 4개\n",
    "        f = A[:, :h]\n",
    "        g = A[:, h:2*h]\n",
    "        i = A[:, 2*h:3*h]\n",
    "        o = A[:,3*h:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f * c_prev + (g * i)\n",
    "        h_next = o * np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, c_next, h_next, f, g, i, o)\n",
    "        \n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 4, 300)            362400    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 300)               721200    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1083901 (4.13 MB)\n",
      "Trainable params: 1083901 (4.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=300, return_sequences=True, input_shape=[4, 1]),\n",
    "    tf.keras.layers.LSTM(300),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28620957b20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(x_data, y_data, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 389ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.3767967],\n",
       "       [5.08792  ],\n",
       "       [6.367443 ],\n",
       "       [7.3045864],\n",
       "       [7.9988074],\n",
       "       [8.522839 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperpameter\n",
    "vocab_size = 10000 #토큰화 최대 단어수(어휘수)\n",
    "embedding_dim = 16 # Embedding layer => output size\n",
    "max_length = 100 #한 문장의 길이(데이터 셋의 길이)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161153 (629.50 KB)\n",
      "Trainable params: 161153 (629.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#LSTM을 사용하지 않음\n",
    "model = tf.keras.Sequential([\n",
    "    #10000 * 16 = 160000\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length ),\n",
    "    tf.keras.layers.GlobalAvgPool1D(), #1차원의 벡터로 변환\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 64)                20736     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184961 (722.50 KB)\n",
      "Trainable params: 184961 (722.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatten ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                20736     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184961 (722.50 KB)\n",
      "Trainable params: 184961 (722.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "굳이 flatten 할 필요 없음을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
