
#### 과적합 발생 원인
- 층이 너무 많은 경우
- 변수가 복잡할 경우
- 테스트 셋과 학습셋이 중복될 때 발생하기도 함

#### 학습셋과 테스트셋
- 은닉층이 늘어나면 어느 정도 까지는 학습셋과 테스트셋의 예측률은 올라가지만 그 지점을 넘어가면 학습셋의 예측률은 높아지지만, 테스트셋의 예측률은 떨어짐 -> 과적합

>국제 논문 (컨퍼런스, 저널 등)
>https://ieeexplore.ieee.org/Xplore/home.jsp
>https://dl.acm.org/
>
>국내 논문
>https://riss.kr

#### 과적합 방지 방법
- Early Stopping
- 가중치 규제
- 데이터 증강
- drop out

Early Stopping: 검증 손실이 감소하지 않는 것 처럼 보일 때(0.01 or 0.001) 훈련을 중단
가중치 규제: 가중치 분포를 균일하게 하는 것. 
- L1 규제: Loss = Cost + Lambda sigma |w| (가중치를 0으로 만든다는 단점)
- L2 규제: Loss = Cost + lambda sigma w2 (가중치를 0으로 만들지 않음-> 주로 이용)
데이터 증강: 소량의 훈련 데이터에서 많은 훈련 데이터를 뽑아내기 (좌우 반전, 회전 등)
drop out: 중간에 있는 node를 랜덤하게 제외 (비율)

#### 테스트 셋 shuffle 하는 이유

```python
from sklearn.model_selection import train_test_split

# 학습 셋과 테스트 셋 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)
```

- 데이터 무작위 성 보장
- 일반화 능력 강화 (과적합을 방지하기 위해)
- 같은 데이터로부터 생성된 샘플의 유사한 분포를 방지
- 교차 검증을 위해 (k-fold)


```python
history = model.fit(X_train, y_train, epochs=200, batch_size=10, verbose=0)

# 2/2 [==============================] - 0s 3ms/step - loss: 0.7792 - accuracy: 0.8049
```

- verbose=0은 학습 과정을 전부 생략하고 최종 1줄만 출력

#### 검증셋
최적의 학습 파라미터를 찾기 위해 학습 과정에서 사용하는 것

#### 모델 업데이트하기
* epoch가 50이면 순전파 50번 역전파 50번을 실시한다는 뜻
* 50번의 epoch 중 최적의 학습이 40번 째에 이루어질 수 있음
  -> epoch마다 모델의 정확도를 기록하면서 저장

##### Epoch 별로 모델 저장하기

```python
# 모델 저장 조건 설정
modelpath = "./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5"
checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)

# 모델 실행
history = model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])
```

* callbacks에 checkpointer를 설정하면 epoch 끝날 때마다 저장됨

##### Early Stopping

```python
early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)

history = model.fit(X_train, y_train, epochs=1000, batch_size=500, validation_split=0.25, verbose=0, callbacks=[early_stopping_callback, checkpointer])
```

* patience 20

